import re
import oracledb
import pandas as pd
import joblib
import random
from collections import Counter
from datetime import datetime

# ì‹œê°í™”ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# ë¨¸ì‹ ëŸ¬ë‹ (ì‚¬ì´í‚·ëŸ°)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.utils import resample

# ==========================================
# 1. í™˜ê²½ ì„¤ì •
# ==========================================
# DB ì ‘ì† ì •ë³´ (ë¡œì»¬ ê°œë°œìš©)
DB_USER = ""
DB_PASSWORD = ""
DB_DSN = ""

# í•œ ì§€ì—­ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ í¸í–¥ë˜ë‹ˆê¹Œ ìµœëŒ€ ê°œìˆ˜ ì œí•œ
MAX_PER_AREA = 300
SEED = 42
random.seed(SEED)

# ìœˆë„ìš° í•œê¸€ í°íŠ¸ ì„¤ì • (ì´ê±° ì•ˆí•˜ë©´ ê·¸ë˜í”„ ê¸€ì ê¹¨ì§)
FONT_PATH = "C:/Windows/Fonts/malgun.ttf"
plt.rc('font', family='Malgun Gothic')
plt.rcParams['axes.unicode_minus'] = False


# ==========================================
# 2. DB ì—°ê²° ìœ í‹¸
# ==========================================
def get_connection():
    return oracledb.connect(user=DB_USER, password=DB_PASSWORD, dsn=DB_DSN)


# ==========================================
# 3. ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (DM í…Œì´ë¸”)
# ==========================================
def fetch_data_from_db():
    print("ğŸ“¡ DBì—ì„œ ë°ì´í„° ê¸ì–´ì˜¤ëŠ” ì¤‘...")
    conn = get_connection()

    # ì˜¤ë¼í´ CLOB íƒ€ì…ì€ ê·¸ëƒ¥ ì½ìœ¼ë©´ ì—ëŸ¬ë‚˜ì„œ Stringìœ¼ë¡œ ë³€í™˜í•´ì¤˜ì•¼ í•¨
    def output_type_handler(cursor, name, default_type, size, precision, scale):
        if default_type == oracledb.CLOB:
            return cursor.var(oracledb.STRING, arraysize=cursor.arraysize)

    conn.outputtypehandler = output_type_handler

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¡°íšŒ
    query = "SELECT DMID, CONTENT, DISASTERTYPE, AREA FROM DM"
    df = pd.read_sql(query, conn)
    conn.close()

    # ì»¬ëŸ¼ëª… ë‹¤ ëŒ€ë¬¸ìë¡œ ë§ì¶°ì¤Œ (ë‚˜ì¤‘ì— í—·ê°ˆë¦¬ì§€ ì•Šê²Œ)
    df.columns = [c.upper() for c in df.columns]
    print(f"ğŸ“¦ ì´ {len(df)}ê±´ ë¡œë“œ ì™„ë£Œ.")
    return df


# ==========================================
# 4. ìœ„í—˜ë„ ë¼ë²¨ë§ (Rule-base)
# ==========================================
def label_risk(content):
    """
    AI í•™ìŠµì„ ìœ„í•œ ì •ë‹µì§€ ë§Œë“¤ê¸°.
    íŠ¹ì • í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ 'ìœ„í—˜', ì•„ë‹ˆë©´ 'ì•ˆì „'ìœ¼ë¡œ ì¼ë‹¨ ë¶„ë¥˜í•¨.
    """
    text = str(content)

    danger_keywords = [
        "í™•ì§„", "ê°ì—¼", "ì „íŒŒ", "ì‚¬ë§", "ëŒ€í”¼", "í­ë°œ", "ë¶•ê´´",
        "ì‚°ë¶ˆ", "ì¹¨ìˆ˜", "ê³ ë¦½", "ìœ„í—˜", "ê°•í’", "ê°•í•œ ë°”ëŒ"
    ]

    for kw in danger_keywords:
        if kw in text:
            return "ìœ„í—˜"  # DANGER

    return "ì•ˆì „"  # SAFE


# ==========================================
# 5. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ì œê±°)
# ==========================================
def clean_text(text):
    text = str(text)

    # ë‚ ì§œ, ì‹œê°„ ê°™ì€ê±´ íŒ¨í„´ ë¶„ì„ì— ë°©í•´ë˜ë‹ˆê¹Œ ì œê±°
    text = re.sub(r"\d{1,2}ì‹œ|\d{1,2}ë¶„|\d{1,2}ì´ˆ", " ", text)
    text = re.sub(r"\d{2,4}ë…„|\d{1,2}ì›”|\d{1,2}ì¼", " ", text)

    # íŠ¹ìˆ˜ë¬¸ìë‘ ê´„í˜¸ë„ ë‹¤ ë‚ ë¦¼
    text = re.sub(r"[\[\]\(\)<>\"']", " ", text)
    text = re.sub(r"[^ê°€-í£0-9a-zA-Z\s]", " ", text)

    # ê³µë°± ì—¬ëŸ¬ê°œëŠ” í•˜ë‚˜ë¡œ
    text = re.sub(r"\s+", " ", text).strip()
    return text


def build_area_patterns(area_series):
    # ì§€ì—­ëª…(ì˜ˆ: ì„œìš¸, ë¶€ì‚°)ì„ í…ìŠ¤íŠ¸ì—ì„œ ì§€ìš°ê¸° ìœ„í•´ íŒ¨í„´ ë¯¸ë¦¬ ë§Œë“¦
    # ì§€ì—­ëª… ë•Œë¬¸ì— AIê°€ í¸í–¥ë˜ëŠ”ê±¸ ë§‰ìœ¼ë ¤ê³  í•¨
    areas = area_series.dropna().astype(str).unique().tolist()
    tokens = set()

    for a in areas:
        for part in re.split(r'[,/]\s*', a):
            part = part.strip()
            if len(part) > 1:
                tokens.add(re.escape(part))

    token_list = sorted(tokens, key=lambda x: len(x), reverse=True)
    patterns = [re.compile(t) for t in token_list]
    return patterns


def remove_area_mentions(text, area_patterns):
    # ë³¸ë¬¸ì—ì„œ ì§€ì—­ëª… ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    s = str(text)
    for p in area_patterns:
        s = p.sub(" ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


# ==========================================
# 6. í•™ìŠµìš© ë°ì´í„°ì…‹ ë§Œë“¤ê¸°
# ==========================================
def prepare_training_dataframe(df, max_per_area=MAX_PER_AREA):
    print("ğŸ› ï¸ í•™ìŠµ ë°ì´í„° ê°€ê³µ ì¤‘... (ì§€ì—­ëª… ì œê±° ë“±)")
    df.columns = [c.lower() for c in df.columns]

    # í…ìŠ¤íŠ¸ ì²­ì†Œ
    df['content'] = df['content'].astype(str).apply(clean_text)
    df['disastertype'] = df['disastertype'].astype(str)

    # ìœ„ì—ì„œ ë§Œë“  ë£°ë² ì´ìŠ¤ í•¨ìˆ˜ë¡œ ìœ„í—˜ë„ ë¼ë²¨ë§
    df['risk_label'] = df['content'].apply(label_risk)

    # ì§€ì—­ëª… íŒ¨í„´ ì¶”ì¶œ
    area_patterns = build_area_patterns(df['area'])

    rows = []
    for _, r in df.iterrows():
        content = r['content']
        dtype = r['disastertype']
        area = r['area'] if pd.notna(r['area']) else "UNKNOWN"
        risk = r['risk_label']

        # 1. ì›ë³¸ ë°ì´í„° ì¶”ê°€
        rows.append({
            'content': content,
            'disastertype': dtype,
            'risk_label': risk,
            'area': area,
            'version': 'original'
        })

        # 2. ì§€ì—­ëª… ì§€ìš´ ë²„ì „ë„ ì¶”ê°€ (ë°ì´í„° ì¦ê°• íš¨ê³¼ + í¸í–¥ ë°©ì§€)
        anon = remove_area_mentions(content, area_patterns)
        # ì§€ì§„, ê°•í’ ë“±ì€ ì§€ì—­ëª…ì´ ì¤‘ìš”í•  ìˆ˜ë„ ìˆì–´ì„œ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ë§Œ
        if anon.strip() and dtype not in ["ì§€ì§„", "ê°•í’", "ëŒ€ì„¤", "ì‚°ë¶ˆ", "ìˆ˜ë„"]:
            rows.append({
                'content': anon,
                'disastertype': dtype,
                'risk_label': risk,
                'area': area,
                'version': 'anon'
            })

    # ë°ì´í„° ì…”í”Œ
    df_all = pd.DataFrame(rows).sample(frac=1, random_state=SEED).reset_index(drop=True)

    # íŠ¹ì • ì§€ì—­ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¢€ ì¤„ì„ (Downsampling)
    final = []
    area_counts = Counter()

    for _, r in df_all.iterrows():
        area = r['area']
        if area_counts[area] < max_per_area:
            final.append(r)
            area_counts[area] += 1

    df_final = pd.DataFrame(final)
    print(f"âœ… ìµœì¢… í•™ìŠµ ë°ì´í„°: {len(df_final)}ê±´ í™•ë³´")

    return df_final


# ==========================================
# 7. ì‹œê°í™” (í™•ì¸ìš©)
# ==========================================
def visualize_data(df):
    print("\nğŸ“Š ë°ì´í„° ë¶„í¬ ì‹œê°í™” ìƒì„± ì¤‘...")

    # ì¬ë‚œ ì¢…ë¥˜ë³„ ë¶„í¬ ê·¸ë˜í”„
    plt.figure(figsize=(10, 5))
    df['disastertype'].value_counts().plot(kind='bar')
    plt.title("ì¬ë‚œ ì¢…ë¥˜ ë¶„í¬")
    plt.tight_layout()
    plt.savefig("class_distribution.png")
    plt.close()

    # ìœ„í—˜ë„ ë¶„í¬ ê·¸ë˜í”„
    plt.figure(figsize=(6, 4))
    df['risk_label'].value_counts().plot(kind='bar')
    plt.title("ìœ„í—˜ë„(ìœ„í—˜/ì•ˆì „) ë¶„í¬")
    plt.tight_layout()
    plt.savefig("risk_distribution.png")
    plt.close()

    # ì›Œë“œí´ë¼ìš°ë“œ (ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ í™•ì¸)
    text = " ".join(df['content'].tolist())
    try:
        wc = WordCloud(font_path=FONT_PATH, width=800, height=400, background_color="white")
        wc.generate(text)
        wc.to_file("wordcloud.png")
    except:
        print("âš ï¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨ (í°íŠ¸ ë¬¸ì œì¼ ìˆ˜ ìˆìŒ)")

    print("ğŸ“ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ (class_distribution.png ë“±)")


# ==========================================
# 8. ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ (í•µì‹¬)
# ==========================================
def train_and_save_models(df):
    print("\nğŸš€ AI ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

    # 1ì°¨ ëª¨ë¸: ì¬ë‚œ ì¢…ë¥˜ ë¶„ë¥˜ (Naive Bayes ì‚¬ìš©)
    # TF-IDFë¡œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë°”ê¾¸ê³  ë¶„ë¥˜ê¸°ì— ë„£ìŒ
    pipeline1 = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('clf', MultinomialNB())
    ])

    print("   -> ì¬ë‚œ ì¢…ë¥˜ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    pipeline1.fit(df['content'], df['disastertype'])
    # â˜… ì €ì¥ íŒŒì¼ëª… ìˆ˜ì • (server.pyë‘ ë§ì¶¤)
    joblib.dump(pipeline1, "model_disaster.pkl")

    # 2ì°¨ ëª¨ë¸: ìœ„í—˜/ì•ˆì „ íŒë³„
    pipeline2 = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=3000)),
        ('clf', MultinomialNB())
    ])

    print("   -> ìœ„í—˜ë„ íŒë‹¨ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    pipeline2.fit(df['content'], df['risk_label'])
    # â˜… ì €ì¥ íŒŒì¼ëª… ìˆ˜ì • (server.pyë‘ ë§ì¶¤)
    joblib.dump(pipeline2, "model_safety.pkl")

    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model_disaster.pkl, model_safety.pkl")

    return pipeline1, pipeline2


# ==========================================
# 9. ë¶„ì„ ê²°ê³¼ DB ì—…ë°ì´íŠ¸
# ==========================================
def save_analysis_results(df, model_type, model_risk):
    conn = get_connection()
    cur = conn.cursor()

    print("\nğŸ’¾ í•™ìŠµëœ ëª¨ë¸ë¡œ ì „ì²´ ë°ì´í„° ì¬ë¶„ì„ & DB ì €ì¥ ì¤‘...")

    # ì „ì²´ ë°ì´í„° ë‹¤ì‹œ ì˜ˆì¸¡í•´ì„œ DBì— ì—…ë°ì´íŠ¸ (DM_ANALYSIS í…Œì´ë¸”)
    for _, row in df.iterrows():
        # 'version'ì´ 'original'ì¸ ê²ƒë§Œ ì €ì¥ (ì¦ê°•ëœ ë°ì´í„°ëŠ” ì œì™¸)
        if row.get('version') != 'original':
            continue

        dmid = row.get('dmid')  # ì›ë³¸ ë°ì´í„° í”„ë ˆì„ì—ì„œ dmid ê°€ì ¸ì™€ì•¼ í•¨ (ì—¬ê¸°ì„  ìƒëµë  ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜)

        # dmidê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤ (ì¦ê°• ë°ì´í„°ì¼ ê²½ìš°)
        if pd.isna(dmid):
            continue

        content = row['content']

        # ì˜ˆì¸¡ ì‹¤í–‰
        pred_type = model_type.predict([content])[0]
        pred_risk = model_risk.predict([content])[0]

        # MERGEë¬¸ìœ¼ë¡œ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
        try:
            cur.execute("""
                MERGE INTO DM_ANALYSIS t
                USING (SELECT :dmid AS dmid FROM dual) s
                ON (t.dmid = s.dmid)
                WHEN MATCHED THEN
                    UPDATE SET t.pred_type = :pt, t.pred_risk = :pr, t.updated = SYSDATE
                WHEN NOT MATCHED THEN
                    INSERT (dmid, pred_type, pred_risk, created)
                    VALUES (:dmid, :pt, :pr, SYSDATE)
            """, {
                "dmid": int(dmid) if pd.notna(dmid) else 0,
                "pt": pred_type,
                "pr": pred_risk
            })
        except Exception as e:
            pass  # ì—ëŸ¬ë‚˜ë„ ì¼ë‹¨ ì§„í–‰

    conn.commit()
    conn.close()

    print("ğŸ“Œ DM_ANALYSIS í…Œì´ë¸” ì—…ë°ì´íŠ¸ ì™„ë£Œ")


# ==========================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# ==========================================
def main():
    print("\n=== ğŸ”¥ SafetyNevi AI í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")

    # 1. DBì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df_db = fetch_data_from_db()

    if df_db.empty:
        print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ë¨¼ì € í•˜ì„¸ìš”.")
        return

    # 2. ë°ì´í„° ê°€ê³µ (ì „ì²˜ë¦¬, ì¦ê°•)
    df_train = prepare_training_dataframe(df_db)

    # 3. ë°ì´í„° ë¶„í¬ í™•ì¸ (ì´ë¯¸ì§€ ì €ì¥)
    visualize_data(df_train)

    # 4. ëª¨ë¸ í•™ìŠµ & ì €ì¥ (.pkl íŒŒì¼ ìƒì„±)
    model_type, model_risk = train_and_save_models(df_train)

    # 5. (ì„ íƒì‚¬í•­) ë¶„ì„ ê²°ê³¼ë¥¼ DBì— ë‹¤ì‹œ ì €ì¥
    # ì£¼ì˜: prepare_training_dataframeì—ì„œ dmidê°€ ìœ ì‹¤ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
    # ì‹¤ì œë¡œëŠ” ì›ë³¸ df_dbë¥¼ ê°€ì§€ê³  ì˜ˆì¸¡í•´ì„œ ì €ì¥í•˜ëŠ”ê²Œ ë” ì •í™•í•¨.
    # ì—¬ê¸°ì„œëŠ” íë¦„ìƒ df_dbë¥¼ ë‹¤ì‹œ í™œìš©
    print("\n--- ì›ë³¸ ë°ì´í„° ì¬ë¶„ì„ ---")
    df_db['content_clean'] = df_db['CONTENT'].apply(clean_text)

    # save_analysis_results í•¨ìˆ˜ë¥¼ df_db(ì›ë³¸) ê¸°ì¤€ìœ¼ë¡œ í˜¸ì¶œí•˜ë„ë¡ ìˆ˜ì •
    # ì»¬ëŸ¼ëª…ì„ ì†Œë¬¸ìë¡œ ë§ì¶°ì„œ ë„˜ê¹€
    df_for_save = df_db.rename(columns={'DMID': 'dmid', 'content_clean': 'content'})
    df_for_save['version'] = 'original'  # ê°•ì œë¡œ ë§ˆí‚¹

    save_analysis_results(df_for_save, model_type, model_risk)

    print("\n=== ğŸ‰ í•™ìŠµ ì™„ë£Œ! ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”. ===")


if __name__ == "__main__":
    main()